#! /usr/bin/env python3

# A quick and dirty python script for backing up Coralogix alerts and dashboards, optionally
# into a git repo.

# Avi 2025-07-22

import logging
import os
import datetime
import requests
import subprocess
import json
import yaml
import sys
import time

logging.basicConfig(encoding='utf-8', level=logging.INFO)
if 'LOGLEVEL' in os.environ:
  logging.getLogger().setLevel(os.environ['LOGLEVEL'])


if "CX_API_KEY" in os.environ:
  api_key = os.environ['CX_API_KEY']
  logging.debug(f"Reading CX_API_KEY from environment; got {api_key}")

if "CX_REGION" in os.environ:
  region = os.environ['CX_REGION']
  logging.debug(f"Reading CX_REGION from environment; got {region}")

if 'CX_BACKUPS_GIT_REPO_URL' in os.environ:
  git_repo_url = os.environ['CX_BACKUPS_GIT_REPO_URL']
  if not 'CX_BACKUPS_DO_GIT' in os.environ:
    os.environ['CX_BACKUPS_DO_GIT'] = 'true'

workdir = os.getcwd()
if 'CX_BACKUPS_WORKDIR' in os.environ:
  workdir = os.environ['CX_BACKUPS_WORKDIR']
elif 'CX_BACKUPS_GIT_REPO_URL' in os.environ:
  workdir = '/tmp/cx-backups/'
else:
  workdir = './cx-backups.d/'

subdir = './'
if 'CX_BACKUPS_SUBDIR' in os.environ:
  subdir = os.environ['CX_BACKUPS_']

if 'CX_BACKUPS_GIT_COMMIT_MESSAGE' in os.environ:
  git_commit_message = os.environ['CX_BACKUPS_GIT_COMMIT_MESSAGE']
else:
  git_commit_message = 'cx-backups ' + datetime.datetime.now().isoformat()

save_dirs = {
  'dashboards':  './dashboards/',
  'alerts': './alerts/',
  'events2metrics': './events2metrics/',
  'parsing-rules': './parsing-rules/'
}

http_endpoints = {
  "AP1": "https://api.ap1.coralogix.in",
  "AP2": "https://api.ap2.coralogixsg.com",
  "AP3": "https://api.ap3.coralogix.com",
  "EU1": "https://api.coralogix.com",
  "EU2": "https://api.eu2.coralogix.com",
  "US1": "https://api.coralogix.us",
  "US2": "https://api.cx498.coralogix.com",
}
http_endpoint = http_endpoints[region]
write_files = 1

def main():

  # Handle restores
  if len(sys.argv) > 1 and sys.argv[1] == 'restore':
    if not len(sys.argv) == 3:
      print("Usage: cx-backups restore <path-to-file>")
      exit();
    file = sys.argv[2]
    print(f"Restoring {file}")
    definition = None
    with open(file, 'r') as f:
      definition = json.load(f)
    if 'alertDef' in definition:
      restore_alert(definition['alertDef'])
    elif 'dashboard' in definition:
      restore_dashboard(definition)
    elif 'e2m' in definition:
      restore_e2m(definition)
    elif 'ruleGroups' in definition:
      print("Restoring of Parsing Rules not supported yet")
    else:
      print("This file doesn't appear to be a cx-backup output")
    exit()

  if 'CX_BACKUPS_GIT_REPO_URL' in os.environ:

    logging.info(f"git cloning '{git_repo_url}' into '{workdir}'")
    git_clone()

  if not os.path.exists(workdir):
    os.makedirs(workdir)
  os.chdir(workdir)

  if 'CX_BACKUPS_DO_GIT' in os.environ and not os.path.exists('.git'):
    git_init()

  if not os.path.exists(subdir):
    os.makedirs(subdir)
  workdir_fullpath = os.getcwd()
  os.chdir(subdir)

  # I don't yet have anything useful to do with the list of e2ms or parsing rules, and since the
  # get_* functions do the writing to disk themselves, these just sit there on their own.
  get_e2ms()
  get_parsing_rule_groups()

  alerts_defs = get_alerts_definitions();
  if 'alertDefs' in alerts_defs:
    files_written = [];
    for alertdef in alerts_defs['alertDefs']:
      alert = get_alert(alertdef['id'])
      filename = alertdef['alertDefProperties']['name'] + '::' + alertdef['id']
      files_written.append( os.path.basename( write_json_to_file(alert, 'alerts', filename)))
    delete_unwritten_files('alerts', files_written)
  else:
    logging.info("No alerts found")

  catalog = get_dashboard_catalog();
  db_folders = catalog['folders']
  db_catalog = catalog['dashboards']
  dashboard_summaries = {}
  files_written = [];
  for dashboard_id in db_catalog:
    board = get_dashboard(dashboard_id)
    summary = summarise_board(board)
    #TODO: Recreate whole hierarchy of folders and subfolder
    if 'folderId' in board['dashboard']:
      folder_id = board['dashboard']['folderId']['value']
      folder_name = db_folders[ folder_id ]['name']
      if not folder_name in dashboard_summaries:
        dashboard_summaries[ folder_name ] = []
      dashboard_summaries[ folder_name ].append(summary)
    else:
      if not '_root_folder' in dashboard_summaries:
        dashboard_summaries[ '_root_folder'] = []
      dashboard_summaries['_root_folder'].append(summary)
    filename = board["dashboard"]["name"] + '::' + board["dashboard"]["id"]
    files_written.append( os.path.basename(write_json_to_file(board, 'dashboards', filename)) )
    write_yaml_to_file(dashboard_summaries, '', 'dashboards')
  delete_unwritten_files('dashboards', files_written)

  os.chdir(workdir_fullpath)
  if 'CX_BACKUPS_DO_GIT' in os.environ:
    git_commit()

  if 'CX_BACKUPS_GIT_REPO_URL' in os.environ:
    git_push()

# Alerts
def get_alert(id):
  alert = api_request('GET', f"/mgmt/openapi/v3/alert-defs/{id}")
  return alert

def get_alerts_definitions():
  definitions = api_request('GET', '/mgmt/openapi/v3/alert-defs?')
  write_json_to_file(definitions, '', 'alert-definitions')
  return(definitions)

def restore_alert(definition):
  old_alert = get_alert(definition['id'])
  if old_alert == None:
    method = 'POST'
    data = definition['alertDefProperties']
  else:
    method = 'PUT'
    data = definition
  response = api_request(method, '/mgmt/openapi/v3/alert-defs', data)
  print(f"Restored alert '{response['alertDef']['alertDefProperties']['name']}' as {response['alertDef']['id']} (previous id was {definition['id']}")

# Dashboards
def get_dashboard(id):
  board = api_request('GET', f"/mgmt/openapi/v1/dashboards/dashboards/{id}")
  return board

def get_dashboard_catalog():
  catalog = api_request('GET', '/mgmt/openapi/v1/dashboards/catalog')
  write_json_to_file(catalog, '', 'dashboard-catalog')
  boards = {}
  folders = {}
  for item in catalog["items"]:
    boards[ item['id'] ] = item['name']
    if 'folder' in item and item['folder'] is not None:
      folders[ item['folder']['id'] ] = item['folder']

  catalog = { "dashboards": boards, "folders": folders}
  return catalog

def restore_dashboard(dashboard):
  dashboard_id = dashboard['dashboard']['id']
  old_dashboard = get_dashboard(dashboard_id)
  if old_dashboard == None:
    method = 'POST'
    data = dashboard
  else:
    method = 'PUT'
    data = dashboard
  response = api_request(method, '/mgmt/openapi/v1/dashboards/dashboards', data)
  if response is not None:
    print(f"Restored dashboard '{dashboard['dashboard']['name']}' as {dashboard_id}")

def summarise_board(dashboard):
  summary = {
    "name": dashboard['dashboard']['name'],
    "description": dashboard['dashboard']['description'],
    "sections": []
  }
  for section in dashboard['dashboard']['layout']['sections']:
    s = {}
    if not section is None and 'options' in section and not section['options'] is None and 'custom' in section['options'] and 'name' in section['options']['custom']:
      s['name'] = section['options']['custom']['name']
    if not section is None and 'options' in section and not section['options'] is None and 'custom' in section['options'] and 'description' in section['options']['custom'] and section['options']['custom']['description'] != 'null':
      s['description'] = section['options']['custom']['description']
    s['rows'] = []
    #s['kind'] = 'section'
    if 'rows' in section:
      for row in section['rows']:
        r = {}
        r['widgets'] = []
        #r['kind'] = 'row'
        if 'widgets' in row:
          for widget in row['widgets']:
            w = {}
            #w['kind'] = 'widget'
            w['queries'] = []
            w['name'] = widget['title']
            # There will only be one 'kind' in any given 'widget', but we do a
            # for here rather than needing a list of possible values for it
            for kind in widget['definition']:
              query_definitions = []

              # Widget types that can have multiple queries (lines, for instance) have a list
              # of query definitions under the key 'queryDefinitions', but those types that can
              # only have one (pie charts) just have a single definition under the key 'query'.
              # Here we normalise those
              if 'queryDefinitions' in widget['definition'][kind]:
                for query in widget['definition'][kind]['queryDefinitions']:
                  query_definitions.append( query )

              if 'query' in widget['definition'][kind]:
                query_definitions.append( widget['definition'][kind])
                for queryDef in query_definitions:
                  q = {}
                  if 'dataModeType' in queryDef:
                    if queryDef['dataModeType'] == 'DATA_MODE_TYPE_ARCHIVE':
                      q['tier'] = 'archive'
                    elif queryDef['dataModeType'] == 'DATA_MODE_TYPE_HIGH_UNSPECIFIED':
                      q['tier'] = 'frequentsearch'
                    else:
                      q['tier'] = 'unknown: ' + queryDef[lang]['dataModeType']
                  else:
                    q['tier'] = ''
                  for lang in queryDef['query']:
                    if lang == 'dataprime':
                      q['query_language'] = 'dataprime'
                      q['query'] = queryDef['query']['dataprime']['dataprimeQuery']['text']
                    elif lang =='metrics':
                      q['query_language'] = 'promql'
                      q['query'] = queryDef['query']['metrics']['promqlQuery']['value']
                    elif lang == 'logs':
                      q['query_language'] = 'lucene'
                      #TODO: Actually learn Python and come back and not do this:
                      if 'luceneQuery' in queryDef['query']['logs'] and queryDef['query']['logs']['luceneQuery'] is not None and 'value' in queryDef['query']['logs']['luceneQuery']:
                        q['query'] = queryDef['query']['logs']['luceneQuery']['value']
                      else:
                        q['query'] = ''
                    elif lang == 'spans':
                      q['query_language'] = 'spans'
                      if 'luceneQuery' in queryDef['query']['spans'] and queryDef['query']['spans']['luceneQuery'] is not None and 'value' in queryDef['query']['spans']['luceneQuery']:
                        q['query'] = queryDef['query']['spans']['luceneQuery']['value']
                    else:
                      logging.error(f"Found unknown query language '{lang}' in dashboard '{dashboard['dashboard']['name']}'")
                      q['error'] = f"Unknown language '{lang}'"
                  w['queries'].append(q)
            r['widgets'].append(w)
        s['rows'].append(r)
    summary['sections'].append(s)
  return summary

# E2M
def get_e2ms():
  list = api_request('GET', '/mgmt/openapi/api/v2/events2metrics')
  if 'e2m' in list:
    for e2m in list['e2m']:
      filename = '::'.join([str(e2m['name']), e2m['id']])
      write_json_to_file(list, 'events2metrics', filename)

def get_e2m(e2m_id):
  #TODO: Docs are wrong here
  e2m = api_request('GET', f"/mgmt/openapi/api/v2/events2metrics/{e2m_id}")
  return(e2m)


def restore_e2m(e2m):
  e2m_id = e2m['e2m'][0]['id']
  print(e2m_id)
  old_e2m = get_e2m(e2m_id)
  print(old_e2m)
  print("=====")
  if old_e2m == None:
    method = 'POST'
    data = e2m['e2m'][0]
  else:
    method = 'PUT'
    data = e2m
  response = api_request(method, '/mgmt/openapi/api/v2/events2metrics', data)
  if response is not None:
    print("Restored e2m '{response['e2m']['name']}' as {response['e2m']['id']}")
  else:
    print("Failed!")

# Parsing Rules

def get_parsing_rule_groups():
  rule_groups = api_request('GET', '/mgmt/openapi/api/v1/rulegroups');
  if 'ruleGroups' in rule_groups:
    for rule_group in rule_groups['ruleGroups']:
      filename = '::'.join([str(rule_group['order']), rule_group['name'], rule_group['id']])
      write_json_to_file(rule_groups, 'parsing-rules', filename )
  return(rule_groups);



# Utils

def api_request(method, path, data = ()):


  if 'CX_API_POLITENESS_DELAY' in os.environ:
    logging.debug(f"Sleeping for {os.environ['CX_API_POLITENESS_DELAY']} seconds")
    time.sleep(int(os.environ['CX_API_POLITENESS_DELAY']))

  url = http_endpoint + path
  logging.debug('Key: ' + api_key)
  logging.debug(f"{method}ing {url}")
  headers = {"Authorization": "Bearer " + api_key}
  if method == 'POST' or method == 'PUT':
    response = requests.request(method, url, json=data, headers=headers)
  elif method == 'GET':
    response = requests.request(method, url, headers=headers)
  else:
    print(f"Unknown method '{method}'")
    exit();

  if response.status_code == 200:
    return json.loads(response.text)
  elif response.status_code == 404:
    return None
  else:
    print("ERROR")
    print("RESPONSE:")
    print(response.text)
    logging.debug("REQUEST DATA")
    logging.debug(data)
    logging.debug("ABORTING")
    exit()

def get_file_path(type, filename, ext, subdir=''):
  dirpath = './'
  if type:
    dirpath += save_dirs[type] + '/'
  if len(subdir) > 0:
    dirpath += '/' + subdir
  filename = filename.replace(' ', '_')
  filename = filename.replace('/', '__')
  filepath = dirpath + '/' + filename + '.' + ext
  return(dirpath, filepath)

def delete_unwritten_files(type, written_filenames, subdir=''):
  if 'CX_BACKUPS_DO_GIT' in os.environ and not 'CX_BACKUPS_NO_DELETE_REMOVED_OBJECTS' in os.environ:
    dirpath = get_file_path(type, '', '', subdir)[0]
    cwd = os.getcwd()
    os.chdir(dirpath)
    for file in os.listdir('.'):
      if os.path.isfile(file) and not file in written_filenames:
        logging.debug(f"Clearing up old file '{file}' in dir 'dirpath'")
        git_rm(file)
    os.chdir(cwd)

def write_yaml_to_file(data, type, filename, subdir=''):
  dirpath,filepath = get_file_path(type, filename, 'yaml', subdir)
  if not os.path.exists(dirpath):
    os.makedirs(dirpath)
  with open(filepath, 'w', encoding='utf-8') as f:
    yaml.dump(data, f, indent=2, sort_keys=True)
  return filepath

def write_json_to_file(data, type, filename, subdir=''):
  dirpath,filepath = get_file_path(type, filename, 'json', subdir)
  if not os.path.exists(dirpath):
    os.makedirs(dirpath)
  with open(filepath, 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=2, sort_keys=True)
  return filepath

# Git shennanigans
# TODO: Any error handling at all would be nice!

def git_clone():
  if os.path.exists(workdir):
    shutil.rmtree(workdir)
  subprocess.run(['git', 'clone', git_repo_url, workdir])

def git_commit():
  subprocess.run(['git', 'add', '.'])
  subprocess.run(['git', 'commit', '-a', '-m', git_commit_message ])

def git_push():
  subprocess.run(['git', 'push'])

def git_init():
  subprocess.run(['git', 'init'])

def git_rm(file):
  subprocess.run(['git', 'rm', file])

main()
